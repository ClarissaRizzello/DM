% !TEX TS-program = pdflatex
% !TEX encoding = UTF-8 Unicode

\documentclass[10pt]{article}

%%% PACKAGES
\usepackage[utf8]{inputenc}
\usepackage{booktabs}
\usepackage{array}
\usepackage{paralist}
\usepackage{verbatim}
\usepackage[T1]{fontenc}
\usepackage[english]{babel}
\usepackage{sectsty}
\usepackage{graphicx}
\usepackage{geometry}
\usepackage{fancyhdr}
\usepackage[nottoc,notlof,notlot]{tocbibind}
\usepackage{amsmath}
\usepackage[table,xcdraw]{xcolor}
\usepackage{subcaption}
\usepackage{caption}
\usepackage{lmodern}
\usepackage[colorlinks=true, linkcolor=blue]{hyperref}
\usepackage{float}
\usepackage{booktabs}
\usepackage{multirow}
\usepackage{amssymb}
\usepackage{microtype}
\usepackage{seqsplit}
\usepackage{longtable}
%%% Margini Personalizzati
\geometry{
  a4paper,
  top=2.5cm,
  bottom=2.5cm,
  left=2.5cm,
  right=2.5cm
}

%%% Header & Footer
\pagestyle{fancy}
\fancyhf{}
\fancyfoot[C]{\thepage}

%%% Stile sezioni
\allsectionsfont{\sffamily\bfseries}

%%% Table of Contents

\begin{document}
\setlength{\parindent}{0pt}    % Nessuna indentazione a inizio paragrafo
\setlength{\parskip}{6pt}

%%% Copertina
\begin{titlepage}
    \centering
    \vspace*{2cm}
    
    \includegraphics[width=0.4\textwidth]{image.jpg}\par
    \vspace{1.5cm}
    
    {\scshape\Large Corso di Data Mining 1 \par}
    
    \vspace{2cm}
    {\huge\bfseries DataMining1\_Report \par}
    
    \vspace{2.5cm}
    {\Large\itshape Clarissa Rizzello\par}
    \vspace{0.5cm}
    {\Large\itshape Lavinia Rotellini\par}
    \vspace{0.5cm}
    {\Large\itshape Rachele Sabbatini\par}
    
    \vfill
\end{titlepage}

\tableofcontents
\newpage

\section{Introduction}
In this study, we conducted a data mining analysis on an IMDb dataset. We began with a data understanding phase, exploring and preparing the dataset’s attributes to gain deeper insights. Subsequently, we applied various clustering algorithms, including K-Means, DBSCAN, and Hierarchical Clustering. Following clustering, we progressed to classification, training multiple models such as K-Nearest Neighbors (KNN), Naïve Bayes, and Decision Trees. Our exploration also extended to regression analysis, and finally, we employed association rule mining techniques, specifically Apriori and FP-Growth algorithms, to identify meaningful patterns within the data.
\section{Data Understanding}
This dataset from IMDb contains information about movies and TV shows, including key attributes such as titles, release years, runtimes, user ratings, awards, and genres.
\\A "soft" data preparation strategy was adopted, as the objective was to create a consistent and coherent baseline to be used across various required tasks (clustering, classification, pattern mining, regression), each of which might necessitate further specific transformations. In this preliminary phase, we handled missing values, identified and corrected outliers, removed irrelevant or redundant features, and conducted general data cleaning operations. All modifications were documented and saved in a new \texttt{.csv} file, serving as the shared starting point for subsequent analyses.\subsection{Data Semantics}
Our datase was composed of 16430 records with 23 descriptive features. The features were both categorical (\textit{qualitative}) and numeric (\textit{quantitative}). Table \ref{tab:feature_description} summarizes the key features of the dataset.
\setlength{\tabcolsep}{6pt} % spazio orizzontale tra colonne
\renewcommand{\arraystretch}{1.3} % altezza riga aumentata per leggibilità

\begin{center} % per centrare la tabella, opzionale
\begin{longtable}{| >{\ttfamily}p{5cm} | p{4cm} | p{7cm} |}
\caption{Feature descriptions and types} \label{tab:feature_description} \\
\hline
\textbf{Feature} & \textbf{Feature Type} & \textbf{Description} \\
\hline
\endfirsthead

\hline
\textbf{Feature} & \textbf{Feature Type} & \textbf{Description} \\
\hline
\endhead

\hline
\multicolumn{3}{r}{\textit{Continued on next page}} \\
\endfoot

\hline
\endlastfoot

originalTitle & Categorical, Nominal & Original title of the work, expressed in the original language. \\
\hline
runtimeMinutes & Numeric (integer) & Primary runtime of the title, expressed in minutes. \\
\hline
isAdult & Categorical, Binary & Binary indicator specifying whether the content is adult (1) or not (0). \\
\hline
startYear & Numeric, Discrete & Release year of the title; for TV series, the start year of the series. \\
\hline
endYear & Numeric, Discrete & TV Series end year, if applicable. \\
\hline
numVotes & Numeric, Discrete & Total number of votes received by the title. \\
\hline
numRegions & Numeric, Discrete & Number of geographic regions where this version of the title is available. \\
\hline
worstRating & Numeric, Interval & Lowest possible rating value assigned to the title, minimum on the rating scale. \\
\hline
bestRating & Numeric, Interval & Highest possible rating value assigned to the title, maximum on the rating scale. \\
\hline
canHaveEpisodes & Categorical, Binary & Indicates whether the title can include episodes. \\
\hline
isRatable & Categorical, Binary & Indicates whether the title can be rated by users. \\
\hline
totalImages & Numeric, Discrete & Total number of images associated with the title’s IMDb page. \\
\hline
totalVideos & Numeric, Discrete & Total number of videos associated with the title’s IMDb page. \\
\hline
totalCredits & Numeric, Discrete & Total number of credits associated with the title. \\
\hline
criticReviewsTotal & Numeric, Discrete & Total number of professional critic reviews. \\
\hline
awardWins & Numeric, Discrete & Total number of awards won by the title. \\
\hline
awardNominationsExcludeWins & Numeric, Discrete & Total number of award nominations excluding wins. \\
\hline
titleType & Categorical, Nominal & Category or format of the title (e.g., movie, short, TV series, episode, video). \\
\hline
rating & Numeric, Ratio & Average IMDb rating of the title. \\
\hline
ratingCount & Numeric, Discrete & Total number of user ratings submitted for the title. \\
\hline
countryOfOrigin & Categorical, Multi-valued Nominal & Country where the title was primarily produced. \\
\hline
genres & Categorical, Multi-valued Nominal & Genres associated with the title. \\
\hline
userReviewsTotal & Numeric, Discrete & Total number of user-written reviews. \\
\hline
\end{longtable}
\end{center}
 Table \ref{tab:feature_description} summarizes the key features of the dataset.
\subsection{Distribution and Statistics}
\begin{figure}[H]
    \begin{minipage}{0.60\textwidth}
        \textbf{\texttt{rating}}\par
        The \texttt{rating} variable represents an average evaluation score for the titles.
        Originally expressed as intervals (e.g., (7, 8]), it was transformed into a numerical value (e.g., 7.5).\par

        \begin{itemize}
            \item \texttt{7.5} is the most populated class (4,822 titles), followed by \texttt{6.5}.
            \item Extreme classes \texttt{0.5} and \texttt{1.5} contain only 67 titles, confirming the low frequency of very low ratings.
            \item The right tail (8–10) is less frequent but still significant.
        \end{itemize}
    \end{minipage}
    \hfill
\begin{minipage}{0.39\textwidth}
    \centering
    \includegraphics[width=0.9\linewidth]{rating_dist_prep.png}
    \vspace{3pt}
    \captionof{figure}{Distribution of Average Rating Classes}
    \label{fig:rating-dist}
\end{minipage}
\end{figure}

\begin{minipage}{0.45\textwidth}
\textbf{\texttt{titleType}}\par
This categorical feature indicates the type of the title. The bar chart in Figure \ref{fig:titleType} shows a clear dominance of types such as \texttt{movie}, followed by \texttt{tvSeries} and \texttt{short}. Numerous aggregated metrics were analyzed for each category in order to highlight structural differences and support further feature selection or segmentation.

One of the most relevant graphs, Figure \ref{fig:type_runtime}, shows the average duration of works for each \texttt{titleType}. A high degree of variability between categories is evident:
\begin{itemize}
    \item \texttt{tvMiniSeries} have the highest average duration, followed by \texttt{movie} and \texttt{tvSpecial}.
    \item Formats such as \texttt{short} and \texttt{tvShort} naturally have shorter durations (around 10--15 minutes).
    \item Error bars indicate notable variance, especially within \texttt{tvMiniSeries}, highlighting strong internal heterogeneity.
\end{itemize}
\end{minipage}
\hfill
\begin{minipage}{0.5\textwidth}
\centering
\includegraphics[width=0.9\linewidth]{titleTyple.png}
\captionof{figure}{Distribution of Title Types}
\label{fig:titleType}
\vspace{0.5cm}
\includegraphics[width=0.9\linewidth]{titleType_runtime.png}
\captionof{figure}{Average Duration by Title Type}
\label{fig:type_runtime}
\end{minipage}

\textbf{Other significant findings}:
\begin{itemize}
    \item The runtime distribution shows that movies and videos have longer average durations and a long right tail. \texttt{tvSpecial} and \texttt{short} are mainly concentrated between 0 and 30 minutes.
    \item \texttt{userReviewsTotal}: \texttt{movie} and \texttt{tvSeries} receive far more user comments compared to other types.
    \item \texttt{numRegions}: \texttt{movie} titles are distributed across more regions on average, suggesting greater dissemination and international coverage.
    \item \texttt{criticReviewsTotal}: \texttt{movie} titles also receive significantly more attention from critics.
    \item \texttt{awardNominationsExcludeWins} and \texttt{awardWins}: \texttt{tvSeries} and \texttt{movie} are the categories with the highest number of recognitions, indicating higher perceived quality or relevance.
\end{itemize}
The \texttt{videoGame} category was removed because it is not consistent with the cinematic or television nature of the other categories. Its inclusion could have introduced semantic noise into the models.\\

\begin{figure}[H]
    \begin{minipage}{0.45\textwidth}
        \textbf{\texttt{countryOfOrigin}}\par
        A statistical summary of this feature revealed the diversity of countries represented in the dataset. Nevertheless, a limited number of nations (e.g., USA, UK, Canada) account for the majority of cases. 


        As shown in Figure \ref{fig:country}, the distribution is highly skewed:
        \begin{itemize}
            \item The United States (US) dominates with over 6,500 titles, followed by the United Kingdom (GB) and Japan (JP).
            \item Other countries have significantly lower representation.
        \end{itemize}
        
        \vspace{2mm}
                To reduce the cardinality and asymmetry of the \texttt{countryOfOrigin} feature, a new variable \texttt{continent} was introduced via ISO mapping using the \texttt{pycountry} library.
                As shown in Figure \ref{fig:continent}, the transformation into continents helps aggregate sparse countries into broader and more interpretable categories.
    \end{minipage}
    \hfill
    \begin{minipage}{0.5\textwidth}
        \centering
        \includegraphics[width=0.9\linewidth]{country.png}
        {\footnotesize \captionof{figure}{Distribution of Titles by Country of Origin}\label{fig:country}}
        \vspace{0.5cm}
        \includegraphics[width=0.9\linewidth]{continent.png}
        {\footnotesize \captionof{figure}{Distribution of Titles by Continent(after exploding multi-continent entries)}\label{fig:continent}}
    \end{minipage}
\end{figure}



\subsection{Data Quality and Correlations}
\subsubsection{Handling Missing Values and Duplicates}
From the initial exploration phases, a significant presence of missing values in various variables emerged. Although \texttt{NaN} values were initially explicitly present only in the \texttt{awardWins} column (2,618 values), the string \texttt{\textbackslash N} was found to recur in multiple fields. After converting this string to \texttt{NaN}, the following missing value percentages were identified:

\begin{itemize}
    \item \texttt{endYear}: \textbf{95.05\%}
    \item \texttt{runtimeMinutes}: \textbf{29.53\%}
    \item \texttt{awardWins}: \textbf{15.93\%}
    \item \texttt{genres}: \textbf{2.32\%}
\end{itemize}

The absence of duplicates was also verified, both absolutely and with respect to the \texttt{originalTitle} column, which proved to be unique.

The following strategies were adopted for handling missing values:

\begin{itemize}
    \item \textbf{\texttt{endYear}}: removed due to the excessive percentage of missing data (~95\%), compromising its utility.
    \item \textbf{\texttt{genres}}: rows with missing values were deleted. Reliable imputation techniques capable of credibly assigning a genre were not deemed feasible.
    \item \textbf{\texttt{awardWins}}: missing values were replaced with \textbf{0}, assuming that the absence of data corresponds to no recorded awards. This choice was supported by the observed distribution, in which 0 predominates.
\end{itemize}


\subsection{Outliers}
During the data cleaning phase, a systematic analysis of outliers was conducted for several continuous numerical features.

Initially, records with implausible durations were removed:
\[
\texttt{runtimeMinutes} \leq 0 \quad \text{or} \quad \texttt{runtimeMinutes} > 300
\]
Even for short works, a duration of at least one minute is reasonable, while titles exceeding 5 hours are exceptionally rare and potentially anomalous.

A custom Python class was then implemented to detect and remove outliers, with two available methods:

\begin{itemize}
    \item \textbf{Interquartile Range (IQR)}: uses quartiles to identify extreme values.
    \item \textbf{Percentiles}: removes data below the $0.1^{\text{th}}$ percentile and above the $99.9^{\text{th}}$ percentile.
\end{itemize}

In our case, the method based on \textit{extreme percentiles} was chosen, proving particularly effective for highly skewed and unbalanced distributions, as observed in the analyzed variables. Using IQR would have resulted in the removal of approximately one-third of the dataset.

\paragraph{Variables Considered}
The columns analyzed for outlier detection were: \texttt{numVotes, userReviewsTotal, num\_ratings, awardNominationsExcludeWins, awardWins, criticReviewsTotal, runtimeMinutes}.

For each variable, the acceptable interval was calculated as:
\[
[x_{\text{min}}, x_{\text{max}}] = [P_{0.1}, P_{99.9}]
\]
Where $P_{0.1}$ and $P_{99.9}$ represent the 0.1st and 99.9th percentiles, respectively. Values outside this range were considered outliers and removed. Figure \ref{fig:outliers-before-after} shows \textit{before vs. after} graphs, with thresholds indicated by dashed lines.

A total of 71 outliers were removed, equivalent to approximately \textbf{0.43\%} of the original dataset. The impact was visually confirmed through the plots below.

\begin{itemize}
    \item Extreme values in \texttt{userReviewsTotal}, \texttt{num\_ratings}, and \texttt{criticReviewsTotal} caused strong distribution skewness.
    \item For \texttt{runtimeMinutes}, a \textbf{group-based} removal logic was applied, using \texttt{titleType} as an aggregation criterion. This accounted for the significant heterogeneity among different formats (e.g., \textit{short}, \textit{movie}, \textit{tvMiniSeries}).
\end{itemize}

\begin{figure}[H]
    \centering
    \begin{subfigure}{0.45\textwidth}
        \centering
        \includegraphics[width=0.85\linewidth]{userReviewsTotal_before_after.png}
        \caption{\texttt{userReviewsTotal}}
    \end{subfigure}
    \hfill
    \begin{subfigure}{0.45\textwidth}
        \centering
        \includegraphics[width=0.85\linewidth]{awardWins_before_after.png}
        \caption{\texttt{awardWins}}
    \end{subfigure}

    \vspace{0.5cm}

    \begin{subfigure}{0.45\textwidth}
        \centering
        \includegraphics[width=0.85\linewidth]{runtimeMinutes_before_after.png}
        \caption{\texttt{runtimeMinutes} by \texttt{titleType}}
    \end{subfigure}

    \caption{Distribution of numerical variables before and after outlier removal (extreme percentiles)}
    \label{fig:outliers-before-after}
\end{figure}

The presence of outliers, particularly high-end values, not only distorts variable scales, but may also compromise the performance of distance-sensitive models. Therefore, selective outlier removal was a crucial step in ensuring a cleaner and more reliable data foundation for downstream analysis. However, in order not to excessively alter the original data distribution, we deliberately chose to retain a limited number of moderate outliers. This allowed us to preserve the dataset's heterogeneity while achieving a reasonable trade-off between robustness and representativeness.

\subsection{Particular Values}
\subsection{Data Transformation}

\subsection{Pairwise Correlations and Elimination of Variables}

To identify redundancy and strengthen the quality of the feature set, a correlation analysis was carried out on the numerical variables retained after preprocessing. The resulting Pearson correlation matrix is shown in Figure~\ref{fig:correlation-matrix}.

\begin{figure}[H]
\centering
\includegraphics[width=0.85\linewidth]{correlation_matrix.png}
\caption{Correlation matrix of numerical features}
\label{fig:correlation-matrix}
\end{figure}

The matrix highlights some strong and expected associations: notably, \texttt{awardWins} correlates positively with \texttt{awardNominationsExcludeWins}, and both \texttt{userReviewsTotal} and \texttt{totalReviews} are closely linked to \texttt{criticReviewsTotal} and \texttt{num\_ratings}. These relationships suggest that user engagement, critic presence, and review volume are jointly informative dimensions of title popularity. 

Moderate correlations are also present between production-oriented variables such as \texttt{totalCredits}, \texttt{totalImages}, and \texttt{numRegions}, although no excessive collinearity was observed.

A noteworthy preprocessing step was the consolidation of the highly correlated variables \texttt{numVotes} and \texttt{ratingCount} (previously showing $r > 0.98$), which were replaced by a single aggregated feature named \texttt{num\_ratings}. This transformation retained their informational content while eliminating redundancy. 

Additionally, two columns—\texttt{worstRating} and \texttt{bestRating}—were removed from the dataset due to their negligible variance. Both had almost constant values and therefore lacked any meaningful contribution to model discrimination.

\paragraph{Pairwise Relationships with \texttt{userReviewsTotal}}

To further examine the features most associated with audience participation, we plotted the pairwise relationships between \texttt{userReviewsTotal} and its most correlated variables. The resulting graph is shown in Figure~\ref{fig:pairplot-userreviews}.

\begin{figure}[H]
\centering
\includegraphics[width=0.95\linewidth]{pairplot_user.png}
\caption{Selected pairwise correlations with \texttt{userReviewsTotal}}
\label{fig:pairplot-userreviews}
\end{figure}

From the plot, \texttt{userReviewsTotal} shows a clear upward trend in relation to \texttt{num\_ratings} and \texttt{criticReviewsTotal}, confirming their role as key indicators of content visibility and interest. Some clustering is visible at low values of \texttt{awardNominationsExcludeWins}, with sparse but distinct increases in user reviews among titles with higher nomination counts. The dispersion of values in \texttt{numRegions} also reflects the broader distribution reach of certain titles, especially those with global or multi-market releases.


\section{Clustering}
In this section we provide the results of the clustering analysis, which was performed using the following algorithms:

\begin{itemize}
    \item\textbf{KMeans}, from the family of instance-base clustering algorithms
    \item\textbf{DBScan}, from the family of density-based clustering algorithms
    \item\textbf{HDBScan}, from the family of density-based clustering algorithms
    \item\textbf{Agglomerative (or hierarchical) Clustering}. This last method was applied both with and without the connectivity constraint, and in all of the different versions: Single Link, Average Link, Complete Link and Ward.
    \item\textbf{Kmodes}, to cluster on categorical features.
\end{itemize}

\subsection{Preprocessing towards clustering}
The dataset used for clustering analysis was derived from the one prepared during the data understanding phase, with additional preprocessing tailored to the requirements of each clustering algorithm.
A key consideration in preprocessing was the type of features required by the algorithms. Algorithms such as KMeans, DBSCAN, HDBSCAN, and Agglomerative Clustering operate on numerical features, while KModes is specifically designed for categorical data. Accordingly, the dataset was preprocessed in two different ways to accommodate these distinctions.
For the algorithms requiring numerical input, all features of type 'object' and 'boolean' were removed. These included: \texttt{originalTitle}, \texttt{titleType}, \texttt{canHaveEpisodes}, \texttt{countryOfOrigin}, and \texttt{genres}. Additionally, the decade column—though numerical—was excluded because it represents a categorical concept and is not meaningful in a distance-based analysis. The columns \texttt{userReviewsTotal} and \texttt{criticReviewsTotal} were also dropped, as their information was already incorporated into the \texttt{totalReviews} feature.
The dataset was therefore now composed of the following features: 

The main focus of this analysis was to understand how different types of preprocessing affected the performance of various clustering algorithms, and to identify which preprocessing methods were most appropriate for our dataset. To this end, feature transformations were applied in four different ways:
\begin{itemize}
	\item The first transformation applied MinMax scaling only
	\item The second transformation combined both the MinMax scaling with a logarithmic transformation
	\item The third transformation used Standard scaling alone
	\item The fourth transformation combined Standard scaling with a logarithmic transformation	
\end{itemize}

As for KModes, the dataset was preprocessed by deleting all the numerical features and keeping the following:\texttt{titleType}, \texttt{canHaveEpisodes}, \texttt{countryofOrigin}, \texttt{genres}, \texttt{decade}.

\subsection{Centroid-based Clustering: KMeans}

\begin{table}[ht!]
    \centering
    \begin{tabular}{|c|c|}
    \hline
    Preprocessing & K \\
    \hline
    MinMax & 4 \\
    MinMax+Log & 4 \\
    Standard Scaling & 5 \\
    Standard+Log & 3 \\
    \hline
    \end{tabular}
    \caption{KMeans Results - Elbow Method}
    \label{table:KM_K}
\end{table}
To determine the most appropriate number of clusters for our dataset, we applied the \textbf{Elbow Method}. he results indicated that different preprocessing techniques led to different optimal numbers of clusters, as summarized in Table \ref{table:KM_K}:
\begin{table}[ht!]
    \centering
    \begin{tabular}{|c|c|c|}
    \hline
    \multicolumn{3}{|c|}{KMeans Clustering Results} \\
    \hline
     & SSE & Silhouette Score \\
    \hline
    MinMax & 629.92 & 0.30 \\
    MinMax \& Log & 1723.69 & 0.17 \\
    Standard Scaling & 74678.86 & 0.24 \\
    Standard \& Log & 85385.59 & 0.24 \\
    \hline
    \end{tabular}
    \caption{KMeans Clustering Results}
    \label{table:KM}
\end{table}
However, the clustering results across these methods were generally suboptimal and varied significantly. Table \ref{table:KM} shows the Sum of Squared Errors (SSE) and silhouette scores for each preprocessing approach:
These results clearly indicate that our dataset is not well-suited for clustering, as evidenced by the relatively high SSE values and silhouette scores far from the ideal benchmark of 1. Low silhouette scores suggest poor cluster cohesion and separation, implying that the inherent structure of the data does not strongly support meaningful clusters.
\subsection{Density-based Clustering}

\subsubsection{DBScan}

To determine an appropriate value for the epsilon ($\epsilon$) parameter in the DBSCAN algorithm, we initialized a range of candidate $\epsilon$ values and evaluated how many clusters each generated, along with their corresponding silhouette scores.
\begin{table}[ht!]
    \centering
    \begin{tabular}{|c|c|c|}
    \hline
    \multicolumn{3}{|c|}{DBScan Clustering Results} \\
    \hline
     Preprocessing & $\epsilon$ & Silhouette Score \\
    \hline
    MinMax & 0.3 & 0.57 \\
    MinMax \& Log & 0.3 & 0.15 \\
    Standard Scaling & 0.1 & -0.38 \\
    Standard \& Log & 0.1 & 0.92 \\
    \hline
    \end{tabular}
    \caption{DBScan Clustering Results}
    \label{table:DB}
\end{table}
The \textit{min\_samples} parameter was fixed at 3 throughout the experiments. The results are summarized in Table  \ref{table:DB}.
While the silhouette scores suggest that DBSCAN outperforms the previously tested clustering methods—especially with Standard Scaling combined with the logarithmic transformation (silhouette score of 0.92)—a closer visual inspection revealed significant limitations. In all cases, DBSCAN identified one dominant cluster containing nearly all instances, alongside a few very small clusters. Moreover, the identified clusters showed substantial overlap, making them difficult to distinguish clearly. These observations indicate that, despite promising silhouette scores, DBSCAN may not be well-suited for our dataset.
\begin{figure}[ht!]
    \centering
    \includegraphics[width=0.4\linewidth]{3d_Dbscan.png}
    \caption{An example of the DBScan application on the dataset with standard scaling and logarithmic transformation}
    \label{fig:3d_Dbscan}
\end{figure}

\subsubsection{HDBScan}

Since DBSCAN yielded unsatisfactory results, we extended our analysis to include HDBSCAN, a more advanced density-based clustering algorithm. HDBSCAN is designed to handle datasets with clusters of varying density and can automatically determine the number of clusters. The aim was to assess whether a more flexible approach could better capture underlying structures in our data.

\begin{table}[ht!]
    \centering
    \begin{tabular}{|c|c|}
    \hline
    \multicolumn{2}{|c|}{HDBScan Clustering Results} \\
    \hline
     Preprocessing & Silhouette Score \\
    \hline
    MinMax & -0.16 \\
    MinMax \& Log & -0.05 \\
    Standard Scaling & -0.17 \\
    Standard \& Log & 0.05 \\
    \hline
    \end{tabular}
    \caption{HDBScan Clustering Results}
    \label{table:HDB}
\end{table}
However, as shown in Table \ref{table:HDB}, the silhouette scores remain low across all preprocessing variants, confirming that our dataset does not exhibit the kind of density-based structure that these algorithms rely on.

\subsection{Agglomerative Clustering} 
Hierarchical clustering was applied using four different linkage criteria: \textit{complete linkage}, \textit{single linkage} , \textit{group average}, and \textit{Ward’s method}. Initially, the clustering was performed without a connectivity constraint; however, upon further analysis, incorporating the constraint led to more coherent and better-separated clusters, as reflected in improved silhouette scores.\newline
To evaluate the optimal number of clusters, we tested a range of values for \textit{k}, from 2 to 10, and monitored the corresponding silhouette scores to assess clustering quality. The analysis revealed a consistent trend across all linkage methods: after reaching three clusters, there was a notable decline in silhouette scores — dropping from approximately 0.60 to around 0.10. This pattern suggests that increasing the number of clusters beyond three leads to partitions that are less meaningful and less well-separated, according to the internal validation metrics. \newline
These findings indicate that hierarchical clustering can provide moderately strong results when configured carefully, but the data’s structure likely supports only a limited number of distinct clusters.

\begin{table}[ht!]
    \centering
    \begin{tabular}{|c|c|c|}
    \hline
    \multicolumn{3}{|c|}{Hierarchical Clustering Results} \\
    \hline
    Preprocessing & Algorithm & Silhouette Score \\
    \hline
    MinMax & Average Link & 0.70 \\
    MinMax \& Log & Average Link & 0.50 \\
    Standard Scaling & Complete Linkage & 0.92 \\
    Standard \& Log & Average Linkage & 0.71 \\
    \hline
    \end{tabular}
    \caption{Hierarchical Clustering Results}
    \label{table:Agg_Clustering}
\end{table}
Table \ref{table:Agg_Clustering} summarizes the best silhouette scores achieved across the different hierarchical clustering methods and preprocessing techniques. \newline
Although the silhouette scores initially appeared promising, further visual inspection revealed that the resulting clusters do not reflect meaningful groupings within the data. Specifically, all hierarchical clustering configurations — regardless of linkage method or preprocessing — produced one dominant cluster containing the vast majority of instances, and a few very small clusters with little to no data points. \newline
This pattern is illustrated in the figures below. In each case, the clustering results are heavily imbalanced, with poor separation between clusters. As a result, despite seemingly good silhouette values in some cases, the clusters lack interpretability and real-world relevance, suggesting that hierarchical clustering is not well-suited to this dataset.
\begin{figure}[ht!]
    \centering
    \begin{subfigure}[b]{0.45\textwidth}
        \centering
        \includegraphics[width=\textwidth]{avglink_minmax.png}
        \caption{Average Linkage on MinMax Scaled Data}
        \label{fig:img1}
    \end{subfigure}
    \hfill
    \begin{subfigure}[b]{0.45\textwidth}
        \centering
        \includegraphics[width=\textwidth]{avglink_minmaxlog.png}
        \caption{Average Linkage on MinMax Scaled + Log Transformed Data}
        \label{fig:img2}
    \end{subfigure}
    \caption{Comparison of clustering results on MinMax-scaled datasets}
    \label{fig:confronto_MinMax}
\end{figure}

\begin{figure}[ht!]
    \centering
    \begin{subfigure}[b]{0.45\textwidth}
        \centering
        \includegraphics[width=\textwidth]{complink_std.png}
        \caption{Complete Linkage on Standard Scaled Data}
        \label{fig:img3}
    \end{subfigure}
    \hfill
    \begin{subfigure}[b]{0.45\textwidth}
        \centering
        \includegraphics[width=\textwidth]{avglink_STDLOG.png}
        \caption{Average Linkage on Standard Scaled + Log Transformed Data}
        \label{fig:img4}
    \end{subfigure}
    \caption{Comparison of clustering results on Standard-scaled datasets}
    \label{fig:confronto_STD}
\end{figure}

\subsection{Discussion of the results}
The most meaningful partition was achieved using KMeans on the MinMax-scaled dataset. Although this method did not yield the highest silhouette score, as previously discussed, a higher silhouette score did not consistently result in more interpretable or useful clustering within our dataset. Therefore, we selected this configuration for a deeper interpretative analysis.
Our initial focus was on understanding how KMeans utilizes the available features, as visualized in Figure \ref{fig:km_f}
\begin{figure}[ht!]
    \centering
    \includegraphics[width=0.4\linewidth]{kmeans_features.png}
    \caption{Feature usage by KMeans clustering}
    \label{fig:km_f}
\end{figure}

As shown, KMeans places more emphasis on certain features, while largely ignoring others. Notably, features such as\texttt{awardWins}, \texttt{totalImages}, \texttt{totalVideos} and \texttt{totalCredits} appear to contribute less to the clustering process. This is consistent with their distribution in the dataset: they are sparse and dominated by zero values, with only occasional peaks, making them less informative overall.
\begin{figure}[ht!]
        \centering
        \includegraphics[width=0.4\linewidth]{3d_kmeans.png}
        \caption{3d Scatter Plot of the KMeans clusters}
        \label{fig:3d_km}
\end{figure}

To better understand the spatial distribution of the resulting clusters, a 3D scatter plot was generated using three of the most informative features:  \texttt{runtimeMinutes}, \texttt{awardWins} and \texttt{startYear} (Figure \ref{fig:3d_km}).

The clusters appear tightly grouped with significant overlap, showing no strong visual separation in feature space. Nevertheless, some semantic distinctions can still be inferred:
\begin{itemize}
    \item\textbf{Orange cluster}: composed of heterogeneous products with low \texttt{awardWins},and generally older \texttt{startYear} values. These items are likely forgotten or less popular older media, as this group was also associated with low numbers of ratings in earlier analyses.
    \item\textbf{Green cluster}: also primarily older products, but with more variation in length and higher \texttt{awardWins}. This group likely represents critically acclaimed older media, distinguishing itself from the orange cluster in terms of critical reception.
    \item\textbf{Blue cluster}: contains mostly newer, shorter productions with a broad range of \texttt{awardWins}. It can be interpreted as representing modern television and short-format productions with mixed reception. 
    \item\textbf{Pink cluster}: characterized by longer runtimes, likely indicating films or feature-length content. These products are mostly recent and include both highly acclaimed works and those with fewer awards, suggesting a general grouping of newer movies and long-form media.
\end{itemize}
To support these interpretations, Figure \ref{fig:distrib_knn} shows the distribution of titleType across the clusters:
\begin{figure}[ht!]
        \centering
        \includegraphics[width=0.4\linewidth]{distribution_kn.png}
        \caption{Distribution of the title types in the different clusters}
        \label{fig:distrib_knn}
\end{figure}

\section{Classification}
Classification is a supervised machine learning technique used to predict categorical outcomes. It involves training a model on labeled data, where each instance is associated with a known class. The model learns to recognize patterns from this training data and is then able to classify new, unseen inputs into predefined categories. \newline
For our classification task, we experimented with three supervised learning algorithms: K-Nearest Neighbors (KNN), Naive Bayes, and Decision Tree. The goal was to predict the type of media product based on various features, with the target variable being \texttt{titleType}. Each model was trained on a designated training set and evaluated on a separate test set.

\subsection{Data preparation}
To ensure consistency and comparability across classifiers, we used a common dataset for all models. However, specific preprocessing steps were applied where appropriate to optimize performance. \newline
The features selected for model training were: \texttt{AwardWins}, \texttt{\seqsplit{awardNominationsExcludeWins}}, \texttt{numRegion}, \texttt{genres}, \texttt{TotalReviews}, \texttt{decade}, \texttt{continent}, \texttt{rating\_category}, \texttt{popularity\_category}, \newline
\texttt{\seqsplit{runtime\_category}}

The last three — rating\_category, popularity\_category, and runtime\_category — were derived by binning the original continuous variables: rating, num\_rating, and runtimeMinutes respectively. The remaining numeric features were normalized using a logarithmic transformation followed by MinMax scaling to ensure all input values were on a comparable scale. \newline
The target variable, titleType, was refined to focus on cinema and television products. We excluded categories such as video games, and merged several low-frequency or similar TV-related categories (tvMiniSeries, tvSpecial, and tvShort) into a consolidated class labeled otherTvProducts. This decision was made to address class imbalance and improve the generalizability of our models.

\subsection{KNN}
The first model applied was the K-Nearest Neighbors (KNN) algorithm, a non-parametric supervised learning method that classifies data points based on the majority class among their \textit{k} nearest neighbors in the training set. KNN makes no assumptions about the underlying data distribution and is particularly effective in cases where similar instances belong to the same class. \newline
To optimize model performance, we employed GridSearch to determine the best hyperparameters. The optimal configuration was:
\begin{itemize}
    \item \textbf{Distance metric} =\texttt{cityblock}
    \item \textbf{Number of neighbors (k)} = 5
    \item \textbf{Weights strategy} =\texttt{distance}
\end{itemize}
Given the presence of class imbalance in the dataset, we selected balanced accuracy as the primary scoring metric during hyperparameter tuning. The model achieved an accuracy score of 0.76 on the training set. \newline
While accuracy is easy to interpret, it may not reflect true model performance in the presence of class imbalance. For this reason, we also evaluated the model using precision, recall, and the F1-score for a more balanced perspective.
\begin{figure}[ht]
    \centering
    \begin{minipage}{0.48\textwidth}
        \centering
        \includegraphics[width=\linewidth]{Report_knn.png}
        \caption{KNN Classification Report}
        \label{fig:knn_report}
    \end{minipage}
    \hfill
    \begin{minipage}{0.48\textwidth}
        \centering
        \includegraphics[width=\linewidth]{rocknn.png}
        \caption{KNN ROC Curve}
        \label{fig:knn_roc}
    \end{minipage}
\end{figure}
As shown in Figure \ref{fig:knn_report}, model performance varies across classes, which is expected due to the unequal distribution of samples. Despite this, the model achieved a solid accuracy of 0.77 on the test set, indicating good overall performance. \newline
The ROC curves in Figure \ref{fig:knn_roc} display, for each class, the trade-off between the True Positive Rate (TPR) — the proportion of actual positives correctly predicted — and the False Positive Rate (FPR) — the proportion of negatives incorrectly predicted as positives. This visualization is particularly valuable in multiclass classification tasks, as it reveals how well the model distinguishes each class from the rest.\newline
On average, the model demonstrated a high TPR-to-FPR ratio of 0.88, suggesting that it is able to reliably identify the correct class in most cases. This indicates that the KNN classifier performs robustly across the diverse categories in the dataset, especially when using the tuned hyperparameters and an appropriate distance metric.

\subsection{Naïve Bayes}
In this section, we examine the performance of our model using the Naïve Bayes classifier, a probabilistic approach grounded in Bayesian statistics. Naïve Bayes refers to a family of simple algorithms for constructing classifiers—models that assign class labels to instances represented as vectors of feature values, with labels drawn from a finite set of categories. \newline
While there is no single algorithm that defines all Naïve Bayes classifiers, they are unified by a key assumption: that the value of each feature is conditionally independent of the others, given the class label. This simplifying assumption forms the core of the model and enables efficient computation of class probabilities.

\begin{figure}[ht]
    \centering
    \begin{minipage}{0.48\textwidth}
        \centering
        \includegraphics[width=\linewidth]{Report_naiveB.png}
        \caption{Naïve Bayes Classification Report}
        \label{fig:nb_report}
    \end{minipage}
    \hfill
    \begin{minipage}{0.48\textwidth}
        \centering
        \includegraphics[width=\linewidth]{rocnb.png}
        \caption{ Naïve Bayes ROC Curve}
        \label{fig:nb_roc}
   \end{minipage}
\end{figure}
We trained our model using the Multinomial Naïve Bayes algorithm and obtained the evaluation results shown in Figure \ref{fig:nb_report}. The model achieved an accuracy of 0.68 on the test set. Among all classes, the \textit{"short"} category was the easiest to predict, consistent with the results from the KNN classifier, achieving an F1-score of 85\%. \newline
Some other classes were not predicted as well; for example, certain categories had F1-scores as low as 0.01 and recall values around 0.02, indicating that the classifier struggled to correctly identify instances from those categories. \newline
The ROC curve shown in Figure \ref{fig:nb_roc} illustrates the model's performance across all classes using a one-vs-rest approach. The curves are generally plotted near the top-left corner, suggesting reasonable discriminative power.\newline
In conclusion, while the Naïve Bayes classifier performed adequately, it did not reach the accuracy level obtained with the KNN algorithm.

\subsection{Decision Tree}
A compelling alternative to probabilistic models like Naïve Bayes and instance-based approaches such as KNN is the Decision Tree classifier. This non-parametric, supervised learning method is particularly valued for its interpretability and ability to model complex decision boundaries without requiring feature scaling. \newline
A Decision Tree builds a hierarchical structure by recursively partitioning the dataset based on feature values, aiming to create subsets that are increasingly homogeneous with respect to the target label. It comprises:
\begin{itemize}
	\item Internal nodes, which test specific feature values.
	\item Branches, representing the outcome of a those tests.
	\item Leaf nodes, which assign a final class label 
\end{itemize}
To fine-tune the model and control its complexity, we focused on three critical hyperparameters: \texttt{min\_samples\_split}, \texttt{min\_samples\_leaf}, \texttt{max\_depth}. Using Randomized Search, we identified the optimal configuration as follows:
\begin{itemize}
    \item \texttt{min\_samples\_split} = 96
    \item \texttt{min\_samples\_leaf} = 31
    \item \texttt{max\_depth} = 12
\end{itemize}
The splitting criterion selected was the Gini index, which measures node impurity and helps separate target classes effectively. To improve generalization and reduce overfitting, we applied Cost-Complexity Pruning (CCP), which simplifies the tree by removing branches with little predictive power. The pruning parameter $\alpha$ controls the trade-off between complexity and accuracy: higher values lead to simpler models, while lower values retain complexity but risk overfitting. Our optimal $\alpha$ was 0.00103, indicating a relatively low level of pruning and a balanced model. \newline
On the training set, accuracy improved slightly by 0.02\%, from 0.62 to 0.64. Using the best parameters found via Randomized Search and the selected $\alpha$, the Decision Tree achieved an accuracy of 0.64 on the test set. 
\begin{figure}[ht]
    \centering
    \begin{minipage}{0.48\textwidth}
        \centering
        \includegraphics[width=\linewidth]{Dt_reportval.png}
        \caption{Decision Tree Classification Report on train set}
        \label{fig:dt_report}
    \end{minipage}
    \hfill
    \begin{minipage}{0.48\textwidth}
        \centering
        \includegraphics[width=\linewidth]{ReportDT.png}
        \caption{Decision Tree Classification Report on test set}
        \label{fig:dt_report}
    \end{minipage}
\end{figure}
Figure \ref{fig:dt_roc} shows the ROC curve, which is consistently high across genres, indicating good discrimination between classes. The Precision-Recall curve, more informative for imbalanced datasets, reports a micro-average precision (AP) of 0.72, reflecting strong overall performance, while the macro-average AP of 0.52 indicates weaker performance on minority classes. This difference arises because the ROC curve is less sensitive to class imbalance, calculating TPR and FPR over all instances regardless of class distribution, whereas the Precision-Recall curve provides a more meaningful evaluation in imbalanced scenarios.
\begin{figure}[ht]
    \centering
    \begin{minipage}{0.48\textwidth}
        \centering
        \includegraphics[width=\linewidth]{RocDT.png}
        \caption{Decision Tree Roc Curve}
        \label{fig:dt_roc}
    \end{minipage}
    \hfill
    \begin{minipage}{0.48\textwidth}
        \centering
        \includegraphics[width=\linewidth]{RecallDT.png}
        \caption{Decision Tree Precision-Recall curve}
        \label{fig:dt_pr}
    \end{minipage}
\end{figure}
\subsection{Classification conclusion}
After evaluating the results of each classification algorithm, we concluded that the K-Nearest Neighbors (KNN) classifier delivered the best performance, achieving an accuracy score of 0.77, followed by the Naïve Bayes classifier with an accuracy of 0.68.
\section{Regression}
For this task two datasets were used, as for classification: the training and the test datasets resulting from the preprocessing phase. Additional preprocessing was applied to both datasets. \newline
Since regression is the task of predicting a continue numerical feature, all non-numerical and categorical columns were dropped, resulting in the following ones: \texttt{num\_ratings}, \texttt{startYear}, \texttt{runtimeMinutes}, \texttt{awardWins}, \texttt{totalImages}, \texttt{totalVideo}, \texttt{totalCredits}, \texttt{\seqsplit{awardNominationsExcludeWins}}, \texttt{numRegions}, \texttt{totalReviews}.

The analysis focused on multivariate regression, at first with just one target variable for which we used a \textit{Linear Regressor} and a \textit{Decision Tree Regressor}, and then multivariate regression with two target variables, using again a \textit{Decision Tree Regressor}.
The variables normalized with MinMax scaling and the logarithmic transformation were used just for the \textit{Linear Regressor}, meanwhile for the \textit{Decision Trees} we kept them as they were to preserve the interpretability of the model. This was done with the second part of the analysis in mind, which is to understand whether the most important features selected by the multivariate-single target \textit{Decision Tree} were useful to better the performance of the \textit{Linear Regressor}. \newline
A dummy regressor was also trained as a tool of comparison, with the \textit{strategy} parameter set to \textit{mean}. The R2 value obtained was of -0.00027 which can be approximated as 0.

\subsection{Results of the Multivariate Regression with One Target Variable}
The target variable chosen to be predicted in the multivariate-single target scenario was the one deemed most interesting to be used, the \textit{rating} variable: this is because predicting the rating of a media could potentially mean being able to predict its success.
The Linear Regressor which used all the variables available had the worst performance overall, not even surpassing the dummy classifier. This was probabily because of a lack of linear relationships in the data and possibly a number of features which was too high and not informative. In table \ref{table:lr_results}, the results:

\begin{table}[h]
    \centering
    \begin{tabular}{|c|c|c|}
    \hline
    \multicolumn{3}{|c|}{Linear Regressor Results} \\
    \hline
    MSE & MAE & R2 \\
    \hline
    0.44 & 0.65 & -19.7 \\
    \hline
    \end{tabular}
    \caption{Linear Regressor Results}
    \label{table:lr_results}
\end{table}

In image \ref{fig:linear_scatterplot}, the relationship between the actual rating, the predicted rating and the number of awardWins is shown: it can be seen how the linear regressor always predicts the same value for all instances.

\begin{figure}[h!]
    \centering
    \includegraphics[width=0.4\linewidth]{linear_scatterplot.png}
    \caption{Scatterplot showing behaviour of Linear Regressor}
    \label{fig:linear_scatterplot}
\end{figure}

As for the Decision Tree used for the multivariate-single target regression, the performance was better than the one of both the \textit{Dummy Classifier} and the \textit{Linear Regressor}. The model was trained using a Grid Search using 10 folds and otpimizing the following parameters: \textit{criterion}, \textit{min\_samples\_split},\textit{min\_sample\_leaf}, \textit{max\_depth} and \textit{ccp\_alpha}. The best values were found to be, respectively: squared\_error, 3, 5, 6 and 0.00.
The results obtained were:
\begin{itemize}
    \item R2: 0.191
    \item MSE: 1.558
    \item MAE: 0.950
\end{itemize}

In figure \ref{fig:tree_plot}, the first tree levels of the Decision Tree plotted: 

\begin{figure}[h]
    \centering
    \includegraphics[width=0.4\linewidth]{tree_plot.png}
    \caption{Plot of the Decision Tree - Multivariate \& Single Variable}
    \label{fig:tree_plot}
\end{figure}

\subsection{Feature Importances for the Linear Regressor}
To perform this second part of the analysis, the first action made was to print out the feature importances of the \textit{Decision Tree Regressor} (DTR). Many trials were conducted to understand how many of the features found by the DTR were most meaningful for the \textit{Linear Regressor}, finding the following were most helpful: \texttt{runtimeMinutes}, \texttt{startYear}, \texttt{num\_ratings}, \texttt{totalCredits}, \texttt{numRegions}. \newline
Overall, the Linear Regressor performance is still not good, but it did get significantly better, as shown in table \ref{table:lr_fi}:

\begin{table}[h]
    \centering
    \begin{tabular}{|c|c|c|}
    \hline
    \multicolumn{3}{|c|}{Linear Regressor with Feature Importance} \\
    \hline
    MSE & MAE & R\^2 \\
    \hline
    0.0106 & 0.073 & 0.0526 \\
    \hline
    \end{tabular}
    \caption{Linear Regressor with Feature Importance}
    \label{table:lr_fi}
\end{table}

\subsection{Results of the Multivariate Regression with Two Target Variables}
This analysis was performed using again a Decision Tree Regressor, which was optimized using a grid search over the same parameter's grid that was used before. This time, the best parameters found were: as for criterion it was absolute error, the \texttt{ccp\_alpha} was 0.01,  \texttt{the max\_depth} was best at None, and the  \texttt{min\_samples\_leaf} and  \texttt{min\_samples\_split} were respectively at 5 and 3. \\
As dependent variables  \texttt{rating} and  \texttt{num\_ratings} were chosen, to continue on the path of predicting the possibile popularity of a product. In table \ref{table:dt_2t} the results:

\begin{table}[h]
    \centering
    \begin{tabular}{|c|c|c|}
    \hline
    \multicolumn{3}{|c|}{Decision Tree-Multivariate Regression with 2 Targets} \\
    \hline
    MSE & MAE & $R^2$ \\
    \hline
    156525714.8 & 461.503 & 0.228 \\
    \hline
    \end{tabular}
    \caption{Decision Tree-Multivariate Regression with 2 Targets}
    \label{table:dt_2t}
\end{table}

Although the $R^2$ is at the highest, the MSE and the MAE are also at their highest, signaling that the model is able to understand some variance in the data but is not capable of predicting the instances properly.

\section{Pattern Mining}

\section{Pattern Mining}
\subsection{Data Preparation}

The \textbf{data preparation} phase for pattern mining was carried out with the aim of selecting and processing only the most semantically relevant features, in order to extract interpretable patterns.
Following an initial exploratory inspection, variables considered uninformative or redundant for the analysis—such as \texttt{originalTitle}, \texttt{totalImages}, \texttt{totalVideos}, \texttt{totalCredits}, and \texttt{canHaveEpisodes}—were removed. Additionally, the label  \texttt{short} from the \texttt{titleType} feature was discarded, as this information was already represented in the \texttt{genres} attribute.

\paragraph{Categorical feature processing.}
Continent tagged as \texttt{Unknown} were excluded to ensure semantic consistency.  
The \texttt{continent} and \texttt{genres} features, being multi-label in nature, were transformed into binary format using the \texttt{MultiLabelBinarizer} class, generating a set of binary columns with coherent prefixes (e.g., \texttt{genre\_Comedy}, \texttt{continent\_Europe}).  
The \texttt{titleType} feature was instead encoded using \texttt{get\_dummies}, producing a standard one-hot representation.

\begin{minipage}{0.58\textwidth}
\paragraph{Transformation of numerical variables.}
A mixed approach was adopted for the numerical features, based on discretization techniques guided by empirical distributions and clustering.

The \texttt{num\_ratings} variable, which was highly skewed, was transformed using a logarithmic function (\texttt{log1p}) and subsequently standardized via \texttt{MinMaxScaler}. It was then discretized using \texttt{KMeans} clustering (\texttt{k=5}). The resulting clusters were ordered based on the median and renamed according to rating volume (\texttt{Very Low} -- \texttt{Very High}).
\end{minipage}
\hfill
\begin{minipage}{0.4\textwidth}
\centering
\includegraphics[width=\linewidth]{Kmeans_num_rating.png}
\captionof{figure}{Scatterplot K-Means \texttt{num\_ratings}}
\label{fig:enter-label}
\end{minipage}



\begin{minipage}{0.35\textwidth}
\centering
\begin{tabular}{lc}
\toprule
\textbf{Class} & \textbf{Count} \\
\midrule
Flop & 5022 \\
Considered by Critics & 2466 \\
Critically Acclaimed & 1580 \\
\bottomrule
\end{tabular}
\captionof{table}{Distribution of the \texttt{success\_class} variable.}
\label{tab:success_class}
\end{minipage}
\hfill
\begin{minipage}{0.6\textwidth}
\paragraph{Definition of the \texttt{success\_class} variable.}
As the dataset lacked a direct indicator of critical success, the target variable \texttt{success\_class} was defined through a rule-based approach combining award data and critic reviews. Titles that received at least one award or one nomination were labeled as \texttt{Critically Acclaimed}. Those without awards or nominations but with at least one critic review were categorized as \texttt{Considered by Critics}. The remaining titles, which had neither reviews nor recognition, were classified as \texttt{Flop}.
\end{minipage}


\paragraph{Logical discretization of additional features.}
To improve the interpretability of the patterns, additional numerical variables were discretized using manually defined thresholds based on empirical criteria:

\begin{itemize}
  \item \texttt{numRegions}: \texttt{Local} if equal to 1, \texttt{Limited} if $\leq$5, \texttt{Moderate} if $\leq$10, and \texttt{International} otherwise;
  \item \texttt{rating}: \texttt{Poor} (0--5), \texttt{Average} (5--6.5), \texttt{Good} (6.5--7.5), \texttt{Excellent} ($>$7.5);
\item \texttt{runtimeMinutes}: discretized into \texttt{Short} ($\leq$60 min), \texttt{Standard} (61–90 min), \texttt{Long} (91–120 min), \texttt{Extended} (121–150 min), and \texttt{Epic} ($>$150 min) based on fixed duration intervals.

\end{itemize}

\subsection{Frequent Pattern Extraction}
The \textit{Apriori} and \textit{FP-Growth} algorithms were employed with varying values of minimum support (\texttt{min\_support}) and minimum itemset cardinality (\texttt{zmin}). The trend in the number of frequent, maximal, and closed itemsets was analyzed as a function of support thresholds, highlighting the exponential growth in the number of patterns with increasing granularity.
\subsubsection{Discussion on Frequent Patterns}

The analysis was conducted on a transactional dataset obtained by transforming the categorical and binary variables of the original dataset, representing each movie as a distinct set of items.

For each parameter combination, the following were generated:
\begin{itemize}
    \item \textbf{Maximal itemsets} (\texttt{target='m'}), which cannot be further extended without reducing support;
    \item \textbf{Closed itemsets} (\texttt{target='c'}), which preserve the maximum amount of information while reducing redundancy.
\end{itemize}

The visualizations obtained reveal that the most frequent patterns are dominated by combinations of film genres, confirming the presence of well-established semantic structures within the dataset.

\begin{figure}[H]
\centering
\begin{minipage}{0.38\textwidth}
  \includegraphics[width=\linewidth]{zmin2.png}
  \caption{Closed and maximal itemsets for \texttt{zmin=2}.}
  \label{fig:itemset_zmin2}
\end{minipage}
\hfill
\begin{minipage}{0.38\textwidth}
  \includegraphics[width=\linewidth]{zmin3.png}
  \caption{Closed and maximal itemsets for \texttt{zmin=3}.}
  \label{fig:itemset_zmin3}
\end{minipage}
\end{figure}

As expected, increasing the support threshold leads to a significant reduction in the number of patterns. Moreover, closed itemsets consistently outnumber maximal ones, as they retain more statistical information. Raising \texttt{zmin} from 2 to 3 further decreases the number of patterns, enhancing readability at the cost of simpler pattern loss.

To compare the efficiency and productivity of the Apriori and FP-Growth algorithms, the number of itemsets generated was evaluated at different support levels and itemset sizes. The results are summarized in Table~\ref{tab:algo_comparison}.

\begin{table}[H]
\centering
\caption{Comparison between Apriori and FP-Growth across support and itemset sizes}
\label{tab:algo_comparison}
\begin{tabular}{|c|c|c|c|}
\hline
\textbf{Support (\%)} & \textbf{Itemset size} & \textbf{Apriori} & \textbf{FP-Growth} \\
\hline
8 & 2 & 170 & 113 \\
20 & 2 & 15 & 399 \\
8 & 3 & 3 & 1896 \\
20 & 3 & 64 & 176 \\
\hline
\end{tabular}
\end{table}

As observed, FP-Growth generates a significantly higher number of complex patterns (3-itemsets), particularly at lower support levels. This indicates a greater ability to explore intricate combinations, uncovering latent structures that Apriori tends to miss. In contrast, Apriori proves more efficient in detecting shorter patterns but lacks depth.
In light of these findings, FP-Growth with 8\% support and \texttt{zmin=3} was selected, as it provides an optimal balance between pattern granularity and semantic coverage, demonstrating superiority in both quantitative and qualitative terms for the purposes of this analysis.


\subsection{Association Rules Extraction}
Association rules were extracted using the FP-Growth algorithm, aimed at generating rules of the form:

\[
\text{Antecedent} \Rightarrow \text{Consequent}
\]

The rules were evaluated using three main metrics:
\begin{itemize}
  \item \textbf{Support}: the absolute or relative frequency of the rule's occurrence in the dataset;
  \item \textbf{Confidence}: the conditional probability of the consequent given the antecedent;
  \item \textbf{Lift}: the ratio between the observed confidence and the expected confidence under statistical independence.
\end{itemize}


The rules listed in Table~\ref{tab:regole_lift_alto} represent some of the most relevant patterns identified during the analysis. These rules were selected based on particularly high \textit{lift} values (greater than 2.5), indicating a strong and non-random association between antecedent and consequent.

\begin{table}[H]
\centering
\caption{Examples of high-lift association rules with predictive potential}
\label{tab:regole_lift_alto}
\begin{tabular}{|p{4.2cm}|p{5.5cm}|c|c|c|}
\hline
\textbf{Consequent} & \textbf{Antecedent} & \textbf{Conf.} & \textbf{Lift} & \textbf{\% Support} \\
\hline
rating\_cluster\_label=Very High N. Ratings & region\_distribution=International, type\_movie & 0.388 & 6.56 & 3.14 \\
\hline
genre\_Mystery & genre\_Crime, genre\_Drama & 0.252 & 5.10 & 2.40 \\
\hline
success\_class=Critically Acclaimed & region\_distribution=International, type\_movie & 0.499 & 2.86 & 4.04 \\
\hline
genre\_Crime & type\_tvEpisode, genre\_Drama, region\_distribution=Local & 0.411 & 3.02 & 5.26 \\
\hline
\end{tabular}
\end{table}

For instance, the first rule shows that movies distributed internationally and labeled as \texttt{type\_movie} have a substantially higher likelihood of receiving a very high number of user ratings. This implies a strong relationship between distribution reach and popularity in terms of visibility and audience approval.

Another rule highlights that internationally distributed \texttt{type\_movie} titles are also frequently labeled as \texttt{Critically Acclaimed}, meaning they receive positive recognition from critics. Such rules are especially relevant for predictive tasks involving the qualitative success of movie titles.\\

Different confidence thresholds were tested to evaluate the trade-off between rule quantity and quality. At \texttt{conf = 30}, 1072 rules were extracted; this dropped to 580 at \texttt{conf = 40}, and just 101 at \texttt{conf = 70}. The threshold of \texttt{conf = 40} was chosen as the best compromise, offering a diverse yet manageable set of rules with sufficient confidence and frequent lift values above 1.5—indicative of non-trivial associations.

Rule quality was assessed jointly through support, confidence, and lift: high support indicates statistical reliability, confidence reflects predictive strength, and lift captures the true associative relevance. Notably, the rule \texttt{success\_class=Critically Acclaimed} $\Leftarrow$ \texttt{region\_distribution=International, type\_movie} achieved a lift of 2.86 and confidence of 0.50, pointing to a strong link between international distribution and critical success.


\begin{table}[H]
\centering
\label{tab:regole_flop}
\begin{tabular}{|p{4cm}|p{5.5cm}|c|c|c|}
\hline
\textbf{Consequent} & \textbf{Antecedent} & \textbf{Conf.} & \textbf{Lift} & \textbf{Support (\%)} \\
\hline
success\_class=Flop & continent\_North America, region\_distribution=Local & 0.689 & 1.24 & 21.55 \\
\hline

success\_class=Flop & rating\_category=Average, region\_distribution=Local & 0.749 & 1.35 & 13.52 \\
\hline

region\_distribution=Local & genre\_Drama, continent\_North America, success\_class=Flop & 0.743 & 1.39 & 6.15 \\
\hline
success\_class=Flop & rating\_category=Average, continent\_North America, region\_distribution=Local & 0.721 & 1.30 & 6.89 \\
\hline

\end{tabular}
    \caption{Examples of association rules with confidence $\geq$ 0.65 and relevant support}
\end{table}

\subsubsection{Discussion on Association Rules}

Due to the dataset’s class imbalance (see \emph{data preparation}), a relatively low confidence threshold (\texttt{$\geqslant$0.2}) was adopted to retain rules potentially useful for minority classes.

\begin{figure}[H]
\centering
\includegraphics[width=0.4\textwidth]{dist_sup_lift_conf.png}
\caption{Distribution of association rule metrics: support, confidence, and lift}
\label{fig:metriche_regole}
\end{figure}

As shown in Figure~\ref{fig:metriche_regole}, most rules exhibit low support (2–6\%), consistent with the dataset's sparsity. Confidence values are typically between 0.2 and 0.5, with few rules exceeding 0.7, indicating limited conditional reliability. Lift values cluster around 1.0, but a significant subset exceeds 1.5—some even beyond 3.0—suggesting the presence of strong and potentially informative co-occurrences.

These results highlight a trade-off: many rules are statistically valid yet moderately reliable. The combination of modest confidence and high lift points to nuanced or partial associations. For downstream use (e.g., in predictive models), rule filtering based on lift and coverage becomes essential to reduce noise.

The subsequent analysis concentrated on rules with consequents involving the target variables \texttt{success\_class} and \texttt{rating\_category}, in order to investigate how patterns of critical and audience reception interrelate.

Overall, these distributions confirm that the FP-Growth algorithm was capable of identifying a diverse set of rules, many of which are statistically meaningful, albeit not always highly reliable. The high number of rules with moderate confidence but lift values above 1 suggests the presence of interesting patterns, though sometimes only partially strong or ambiguous. 
Subsequently, our analysis focused on extracting association rules where the target classes \texttt{success\_class} and \texttt{rating\_category} serve as consequents, in order to explore the relationship between critical reception and audience evaluation.

\begin{itemize}
    \item \textbf{success\_class} captures the level of critical acclaim, inferred from reviews, awards, and nominations;
    \item \textbf{rating\_category} reflects audience appreciation as measured by numerical ratings.
\end{itemize}

For both target variables, rules with confidence $\geq 0.2$ and lift $\geq 1.2$ were selected to ensure a meaningful balance between reliability and statistical significance.

For the class \texttt{success\_class=Flop}, several strong rules emerged:
\begin{itemize}
    \item \texttt{(rating\_category=Average, region\_distribution=Local)} $\Rightarrow$ \texttt{success\_class=Flop}, with confidence $\approx 0.75$ and lift $\approx 1.35$;
    \item \texttt{(type\_tvEpisode, continent\_North America, region\_distribution=Local)} $\Rightarrow$ \texttt{success\_class=Flop}, with confidence $\approx 0.76$ and lift $\approx 1.42$.
\end{itemize}

Similarly, highly informative rules for \texttt{success\_class=Critically Acclaimed} include:
\begin{itemize}
    \item \texttt{(region\_distribution=International, type\_movie)} $\Rightarrow$ \texttt{success\_class=Critically Acclaimed}, with confidence $\approx 0.50$ and lift $\approx 2.86$;
    \item \texttt{(genre\_Drama, type\_movie)} $\Rightarrow$ \texttt{success\_class=Critically Acclaimed}, with confidence $\approx 0.32$ and lift $\approx 1.49$.
\end{itemize}

Regarding audience perception, predictive rules for \texttt{rating\_category} were also examined. For the class \texttt{rating\_category=Good}, coherent co-occurrences emerged, such as:
\begin{itemize}
    \item \texttt{(type\_tvEpisode, continent\_North America, region\_distribution=Local)} $\Rightarrow$ \texttt{rating\_category=Good}, with confidence $\approx 0.55$ and lift $\approx 1.57$;
    \item \texttt{(type\_tvEpisode, region\_distribution=Local)} $\Rightarrow$ \texttt{rating\_category=Good}, with confidence $\approx 0.48$ and lift $\approx 1.53$.
\end{itemize}

In general, these rules show that certain content segments—such as North American local TV series—tend to be well-received by the public but may simultaneously be overlooked by critics (e.g., rules associated with \texttt{Flop}).
This dichotomy between public perception and critical evaluation is further reinforced by rules related to the class \texttt{Considered by Critics}, which display lift values above 1.6 in association with internationally distributed films of moderate popularity (\texttt{rating\_cluster=Medium} or \texttt{High N. Ratings}), albeit with lower confidence compared to rules for the \texttt{Flop} class.

The strongest rules were subsequently applied to the test set by checking whether the rule \textit{antecedents} were fully contained within the transactions. The outcome was positive: \textbf{98\% of the transactions} in the test set triggered at least one rule, with an \textbf{average of 14.5 applicable rules per transaction}.
We analyzed the \textit{top-5} most frequently triggered rules. Some of these, although statistically strong, proved semantically trivial (e.g., \texttt{runtime\_category=Short} $\leftrightarrow$ \texttt{type\_tvEpisode}). Therefore, the focus was shifted to rules with informative \textit{consequents}, such as \texttt{success\_class} and \texttt{rating\_category}.

\begin{table}[htbp]
\centering
\caption{Some Association rules with \texttt{success\_class} as antecedent or consequent}
\label{tab:flop_rules}
\begin{tabular}{|p{5cm}|p{4.5cm}|c|c|c|}
\hline
\textbf{Antecedent} & \textbf{Consequent} & \textbf{Confidence} & \textbf{Lift} \\
\hline
success\_class=Flop & type\_tvEpisode & 0.461 & 1.310  \\
\hline
success\_class=Flop & region\_distribution=Local & 0.691 & 1.289 \\
\hline
region\_distribution=Local & success\_class=Flop & 0.714 & 1.289  \\

\hline
\end{tabular}
\end{table}

\begin{table}[htbp]
\centering
\caption{Predictive rules for the \texttt{rating\_category} class}
\label{tab:rating_good_rules}
\begin{tabular}{|p{6.5cm}|p{3.5cm}|c|c|}
\hline
\textbf{Antecedent} & \textbf{Consequent} & \textbf{Conf.} & \textbf{Lift} \\
\hline
type\_tvEpisode, continent\_North America, region\_distribution=Local & rating\_category=Good & 0.508 & 1.634 \\
\hline

type\_tvEpisode,\newline region\_distribution=Local & rating\_category=Good & 0.492 & 1.583 \\

\hline
type\_tvEpisode, \newline region\_distribution=Local, \newline success\_class=Flop & rating\_category=Good & 0.479 & 1.543 \\
\hline
\end{tabular}
\end{table}

This demonstrates how the identified rules are applied to the test set, showing that they are particularly effective for predicting the majority classes such as \texttt{Flop} for \texttt{success\_class} and \texttt{Good} for \texttt{rating\_category}.


\subsection{Predictive Use of the Rules}
The extracted rules were employed to construct a rule-based classifier, where the prediction for each instance is obtained by selecting, among the applicable rules (i.e., those whose antecedents are contained in the transaction), the one with the highest confidence.

\textbf{For the variable \texttt{success\_class}}, the classifier achieved an accuracy of 67.8\%, with particularly strong performance on the \texttt{Flop} class (precision 0.74, recall 0.93, F1-score 0.82). This result reflects the abundance of high-confidence and high-support rules associated with this category. In contrast, the \texttt{Critically Acclaimed} and \texttt{Considered by Critics} classes were predicted with low accuracy, especially in terms of recall (0.18 and 0.35 respectively), indicating reduced predictive coverage for these categories.

\textbf{For the variable \texttt{rating\_category}}, the overall accuracy reached approximately 49\%. Only the \texttt{Average} (F1-score 0.61) and \texttt{Good} (F1-score 0.51) classes were predicted effectively, while \texttt{Excellent} and \texttt{Poor} were never predicted (recall 0.00), reflecting the absence of strong rules for minority classes. This outcome underscores the impact of class imbalance and limits the model's generalizability.

Beyond accuracy, evaluation metrics such as precision, recall, and F1-score confirmed that the rule-based classifier systematically favors majority classes. Its predictive capacity is notably asymmetric, driven by the greater availability of frequent and confident rules for dominant categories.

In summary, while the model performs adequately on prevalent classes, its performance deteriorates sharply for underrepresented ones—highlighting a key limitation of rule-based systems when applied to imbalanced datasets.


\subsection{Conclusions}

The analysis conducted has demonstrated the effectiveness of pattern mining in extracting interpretable knowledge from complex data within the film domain. Following a thorough phase of data preparation and transformation, the application of both FP-Growth and Apriori algorithms enabled the identification of frequent patterns and association rules characterized by substantial levels of support, confidence, and lift.
The FP-Growth algorithm proved preferable due to its ability to generate a significantly higher number of informative rules, particularly for larger itemsets, while maintaining efficient computational performance.
The derived rules highlighted meaningful relationships between movie features, geographical distribution, genres, and success metrics (both critical and audience-based). In particular, a clear distinction emerged between patterns associated with critical acclaim and those related to audience ratings, underlining the utility of this approach for comparative and predictive analysis.
The rule-based classification system produced mixed results. For the variable \texttt{success\_class}, the model achieved a notable accuracy of 67.8\%, showing strong performance for the majority class \texttt{Flop}. However, predictive performance was limited for minority classes. A similar trend was observed for \texttt{rating\_category}, with an overall accuracy of 49\% and poor coverage of the \texttt{Excellent} and \texttt{Poor} categories.
In summary, the rule-based approach demonstrated good descriptive and interpretative potential but exhibited clear limitations in imbalanced supervised learning contexts. The reliance on empirical itemset frequency poses a structural constraint, making it difficult to produce reliable predictions for underrepresented classes.


\end{document}
